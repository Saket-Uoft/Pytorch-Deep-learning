{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle House Prediction Pytorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoeGto_40d9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfslVi0C6cAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df=pd.read_csv('houseprice.csv',usecols=[\"SalePrice\", \"MSSubClass\", \"MSZoning\", \"LotFrontage\", \"LotArea\",\n",
        "                                         \"Street\", \"YearBuilt\", \"LotShape\", \"1stFlrSF\", \"2ndFlrSF\"]).dropna()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFlseTo86egJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "9755ad76-4838-44d1-8e9a-0d780934a8a3"
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>Reg</td>\n",
              "      <td>2003</td>\n",
              "      <td>856</td>\n",
              "      <td>854</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>Reg</td>\n",
              "      <td>1976</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>IR1</td>\n",
              "      <td>2001</td>\n",
              "      <td>920</td>\n",
              "      <td>866</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>IR1</td>\n",
              "      <td>1915</td>\n",
              "      <td>961</td>\n",
              "      <td>756</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>IR1</td>\n",
              "      <td>2000</td>\n",
              "      <td>1145</td>\n",
              "      <td>1053</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MSSubClass MSZoning  LotFrontage  ...  1stFlrSF 2ndFlrSF SalePrice\n",
              "0          60       RL         65.0  ...       856      854    208500\n",
              "1          20       RL         80.0  ...      1262        0    181500\n",
              "2          60       RL         68.0  ...       920      866    223500\n",
              "3          70       RL         60.0  ...       961      756    140000\n",
              "4          60       RL         84.0  ...      1145     1053    250000\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Waku9qOE6hMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "98f18678-2fec-4674-fd73-2c064e8cce10"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1201 entries, 0 to 1459\n",
            "Data columns (total 10 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   MSSubClass   1201 non-null   int64  \n",
            " 1   MSZoning     1201 non-null   object \n",
            " 2   LotFrontage  1201 non-null   float64\n",
            " 3   LotArea      1201 non-null   int64  \n",
            " 4   Street       1201 non-null   object \n",
            " 5   LotShape     1201 non-null   object \n",
            " 6   YearBuilt    1201 non-null   int64  \n",
            " 7   1stFlrSF     1201 non-null   int64  \n",
            " 8   2ndFlrSF     1201 non-null   int64  \n",
            " 9   SalePrice    1201 non-null   int64  \n",
            "dtypes: float64(1), int64(6), object(3)\n",
            "memory usage: 103.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtorsD1G6lN0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "c025018e-ffbb-4ac0-eb81-441654f40086"
      },
      "source": [
        "### Year Built Feature is converted into the difference of years till current year\n",
        "import datetime\n",
        "datetime.datetime.now().year\n",
        "df['Total Years']=datetime.datetime.now().year-df['YearBuilt']\n",
        "df.drop(\"YearBuilt\",axis=1,inplace=True)\n",
        "df.head()\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Street</th>\n",
              "      <th>LotShape</th>\n",
              "      <th>1stFlrSF</th>\n",
              "      <th>2ndFlrSF</th>\n",
              "      <th>SalePrice</th>\n",
              "      <th>Total Years</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>Pave</td>\n",
              "      <td>Reg</td>\n",
              "      <td>856</td>\n",
              "      <td>854</td>\n",
              "      <td>208500</td>\n",
              "      <td>17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Pave</td>\n",
              "      <td>Reg</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>181500</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>Pave</td>\n",
              "      <td>IR1</td>\n",
              "      <td>920</td>\n",
              "      <td>866</td>\n",
              "      <td>223500</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Pave</td>\n",
              "      <td>IR1</td>\n",
              "      <td>961</td>\n",
              "      <td>756</td>\n",
              "      <td>140000</td>\n",
              "      <td>105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>Pave</td>\n",
              "      <td>IR1</td>\n",
              "      <td>1145</td>\n",
              "      <td>1053</td>\n",
              "      <td>250000</td>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MSSubClass MSZoning  LotFrontage  ...  2ndFlrSF SalePrice Total Years\n",
              "0          60       RL         65.0  ...       854    208500          17\n",
              "1          20       RL         80.0  ...         0    181500          44\n",
              "2          60       RL         68.0  ...       866    223500          19\n",
              "3          70       RL         60.0  ...       756    140000         105\n",
              "4          60       RL         84.0  ...      1053    250000          20\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUnZVURP7TzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cat_features=[\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\"]\n",
        "out_feature=\"SalePrice\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyvVKaGy7ZtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##Label Encoding the Categorical Features\n",
        "lbl_encoders={}\n",
        "for feature in cat_features:\n",
        "    lbl_encoders[feature]=LabelEncoder()\n",
        "    df[feature]=lbl_encoders[feature].fit_transform(df[feature])\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfZ2dmaO70lm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "05ce90a3-f4c1-4b7f-b00b-69fad00ab784"
      },
      "source": [
        "### Stacking categorical features and converting them into tensors\n",
        "cat_features=np.hstack((df[feature].values.reshape(-1,1) for feature in cat_features))\n",
        "cat_features"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5, 3, 1, 3],\n",
              "       [0, 3, 1, 3],\n",
              "       [5, 3, 1, 0],\n",
              "       ...,\n",
              "       [6, 3, 1, 3],\n",
              "       [0, 3, 1, 3],\n",
              "       [0, 3, 1, 3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ecoigg_b8faY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "fbad9743-551c-43f4-cc7f-6708bb9e806b"
      },
      "source": [
        "### Converting Numpy array to Torch tensors\n",
        "cat_features=torch.tensor(cat_features,dtype=torch.int64)\n",
        "cat_features"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[5, 3, 1, 3],\n",
              "        [0, 3, 1, 3],\n",
              "        [5, 3, 1, 0],\n",
              "        ...,\n",
              "        [6, 3, 1, 3],\n",
              "        [0, 3, 1, 3],\n",
              "        [0, 3, 1, 3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbSdJWFd8pU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Creating continuos variables\n",
        "cont_features=[]\n",
        "for i in df.columns:\n",
        "    if i in [\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\",\"SalePrice\"]:\n",
        "        pass\n",
        "    else:\n",
        "        cont_features.append(i)\n"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1jRS5o98uJx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "d8a3ab63-6fa8-4676-87b1-4e6a9738416b"
      },
      "source": [
        "### Stacking continuous variable to a tensor\n",
        "cont_values=np.stack([df[feature].values for feature in cont_features],axis=1)\n",
        "cont_values=torch.tensor(cont_values,dtype=torch.float)\n",
        "cont_values"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   65.,  8450.,   856.,   854.,    17.],\n",
              "        [   80.,  9600.,  1262.,     0.,    44.],\n",
              "        [   68., 11250.,   920.,   866.,    19.],\n",
              "        ...,\n",
              "        [   66.,  9042.,  1188.,  1152.,    79.],\n",
              "        [   68.,  9717.,  1078.,     0.,    70.],\n",
              "        [   75.,  9937.,  1256.,     0.,    55.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xFN4Zsp0zEF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "5e14e605-fa62-4835-ccba-e49e7d4a615b"
      },
      "source": [
        "### Converting target variable into Torch tensors\n",
        "y=torch.tensor(df['SalePrice'].values,dtype=torch.float).reshape(-1,1)\n",
        "y"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[208500.],\n",
              "        [181500.],\n",
              "        [223500.],\n",
              "        ...,\n",
              "        [266500.],\n",
              "        [142125.],\n",
              "        [147500.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arVA0S9X6gAH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f838046d-5879-44bc-c231-050aa559212b"
      },
      "source": [
        "#### Finding the number of unique values for each categorical feature\n",
        "cat_dims=[len(df[col].unique()) for col in [\"MSSubClass\", \"MSZoning\", \"Street\", \"LotShape\"]]\n",
        "cat_dims"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15, 5, 2, 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVTYDXuc_8SP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9b30e91d-80c9-4286-ea3e-c77e7392447b"
      },
      "source": [
        "### Finding the desired embedding dimension using some thumb rules\n",
        "embedding_dim= [(x, min(50, (x + 1) // 2)) for x in cat_dims]\n",
        "embedding_dim"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(15, 8), (5, 3), (2, 1), (4, 2)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qno8Vb5c9M3z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "55c72f1a-fa16-492e-c031-04410f2dfb07"
      },
      "source": [
        "embed_representation=nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
        "embed_representation"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModuleList(\n",
              "  (0): Embedding(15, 8)\n",
              "  (1): Embedding(5, 3)\n",
              "  (2): Embedding(2, 1)\n",
              "  (3): Embedding(4, 2)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp7FZYdv94uQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "52dd595d-645e-4bfd-a843-118661daefb7"
      },
      "source": [
        "### Extracting the embedding in form of tensors\n",
        "embedding_val=[]\n",
        "for i,e in enumerate(embed_representation):\n",
        "    embedding_val.append(e(cat_features[:,i]))\n",
        "embedding_val\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[ 0.6479,  0.4738,  0.7491,  ...,  1.3382,  0.5288,  1.1253],\n",
              "         [-1.6160,  1.8831, -0.9623,  ..., -1.3363, -2.4629,  0.9874],\n",
              "         [ 0.6479,  0.4738,  0.7491,  ...,  1.3382,  0.5288,  1.1253],\n",
              "         ...,\n",
              "         [ 1.4524,  0.5377,  0.5810,  ...,  0.3222, -0.2256,  0.3642],\n",
              "         [-1.6160,  1.8831, -0.9623,  ..., -1.3363, -2.4629,  0.9874],\n",
              "         [-1.6160,  1.8831, -0.9623,  ..., -1.3363, -2.4629,  0.9874]],\n",
              "        grad_fn=<EmbeddingBackward>), tensor([[-0.1309, -1.3461, -1.2227],\n",
              "         [-0.1309, -1.3461, -1.2227],\n",
              "         [-0.1309, -1.3461, -1.2227],\n",
              "         ...,\n",
              "         [-0.1309, -1.3461, -1.2227],\n",
              "         [-0.1309, -1.3461, -1.2227],\n",
              "         [-0.1309, -1.3461, -1.2227]], grad_fn=<EmbeddingBackward>), tensor([[-0.0908],\n",
              "         [-0.0908],\n",
              "         [-0.0908],\n",
              "         ...,\n",
              "         [-0.0908],\n",
              "         [-0.0908],\n",
              "         [-0.0908]], grad_fn=<EmbeddingBackward>), tensor([[ 1.1527, -0.1325],\n",
              "         [ 1.1527, -0.1325],\n",
              "         [ 2.1352,  0.4736],\n",
              "         ...,\n",
              "         [ 1.1527, -0.1325],\n",
              "         [ 1.1527, -0.1325],\n",
              "         [ 1.1527, -0.1325]], grad_fn=<EmbeddingBackward>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdgoeDd29vfm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "b6a67448-8ed6-4ef1-9581-aeaa387713ab"
      },
      "source": [
        "### Concatinating all the embedding for every single record\n",
        "z=torch.cat(embedding_val,1)\n",
        "z"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6479,  0.4738,  0.7491,  ..., -0.0908,  1.1527, -0.1325],\n",
              "        [-1.6160,  1.8831, -0.9623,  ..., -0.0908,  1.1527, -0.1325],\n",
              "        [ 0.6479,  0.4738,  0.7491,  ..., -0.0908,  2.1352,  0.4736],\n",
              "        ...,\n",
              "        [ 1.4524,  0.5377,  0.5810,  ..., -0.0908,  1.1527, -0.1325],\n",
              "        [-1.6160,  1.8831, -0.9623,  ..., -0.0908,  1.1527, -0.1325],\n",
              "        [-1.6160,  1.8831, -0.9623,  ..., -0.0908,  1.1527, -0.1325]],\n",
              "       grad_fn=<CatBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MVOWBb8-X4O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "3e0fa875-bd16-440b-f496-8751b521d8ac"
      },
      "source": [
        "### Applying droput and getting the final embeddings\n",
        "dropout=nn.Dropout(0.4)\n",
        "final_embeddings=droput(z)\n",
        "final_embeddings"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0798,  0.0000,  1.2485,  ..., -0.1514,  0.0000, -0.2208],\n",
              "        [-2.6933,  0.0000, -1.6039,  ..., -0.1514,  1.9211, -0.2208],\n",
              "        [ 1.0798,  0.0000,  1.2485,  ..., -0.1514,  3.5587,  0.7893],\n",
              "        ...,\n",
              "        [ 0.0000,  0.0000,  0.9684,  ..., -0.1514,  0.0000, -0.2208],\n",
              "        [-2.6933,  3.1385, -0.0000,  ..., -0.1514,  1.9211, -0.0000],\n",
              "        [-2.6933,  3.1385, -1.6039,  ..., -0.0000,  1.9211, -0.0000]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjccwEb_1ulc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Making a class of feedforward neural network using pytorch\n",
        "class FeedForwardNN(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, n_cont, out_sz, layers, p=0.5):\n",
        "        super().__init__()\n",
        "        self.embeds = nn.ModuleList([nn.Embedding(inp,out) for inp,out in embedding_dim])\n",
        "        self.emb_drop = nn.Dropout(p)\n",
        "        self.bn_cont = nn.BatchNorm1d(n_cont)\n",
        "        \n",
        "        layerlist = []\n",
        "        n_emb = sum((out for inp,out in embedding_dim))\n",
        "        n_in = n_emb + n_cont\n",
        "        \n",
        "        for i in layers:\n",
        "            layerlist.append(nn.Linear(n_in,i)) \n",
        "            layerlist.append(nn.ReLU(inplace=True))\n",
        "            layerlist.append(nn.BatchNorm1d(i))\n",
        "            layerlist.append(nn.Dropout(p))\n",
        "            n_in = i\n",
        "        layerlist.append(nn.Linear(layers[-1],out_sz))\n",
        "            \n",
        "        self.layers = nn.Sequential(*layerlist)\n",
        "\n",
        "      \n",
        "    def forward(self, x_cat, x_cont):\n",
        "        embeddings = []\n",
        "        for i,e in enumerate(self.embeds):\n",
        "            embeddings.append(e(x_cat[:,i]))\n",
        "        x = torch.cat(embeddings, 1)\n",
        "        x = self.emb_drop(x)\n",
        "        \n",
        "        x_cont = self.bn_cont(x_cont)\n",
        "        x = torch.cat([x, x_cont], 1)\n",
        "        x = self.layers(x)\n",
        "        return x  "
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUI8swPp14Oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=FeedForwardNN(embedding_dim,len(cont_features),1,[100,50],p=0.4)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq22svUZ18GU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_function=nn.MSELoss()\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SvowuTI1-7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=1200\n",
        "test_size=int(batch_size*0.15)\n",
        "train_categorical=cat_features[:batch_size-test_size]\n",
        "test_categorical=cat_features[batch_size-test_size:batch_size]\n",
        "train_cont=cont_values[:batch_size-test_size]\n",
        "test_cont=cont_values[batch_size-test_size:batch_size]\n",
        "y_train=y[:batch_size-test_size]\n",
        "y_test=y[batch_size-test_size:batch_size]"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj_Dy0w62C7y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "27b74fc1-fd9c-4976-ae09-5ec4c59b715e"
      },
      "source": [
        "epochs=5000\n",
        "final_losses=[]\n",
        "for i in range(epochs):\n",
        "    i=i+1\n",
        "    y_pred=model(train_categorical,train_cont)\n",
        "    loss=torch.sqrt(loss_function(y_pred,y_train)) ### RMSE\n",
        "    final_losses.append(loss)\n",
        "    if i%10==1:\n",
        "        print(\"Epoch number: {} and the loss : {}\".format(i,loss.item()))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch number: 1 and the loss : 200496.84375\n",
            "Epoch number: 11 and the loss : 200493.78125\n",
            "Epoch number: 21 and the loss : 200489.65625\n",
            "Epoch number: 31 and the loss : 200483.125\n",
            "Epoch number: 41 and the loss : 200473.9375\n",
            "Epoch number: 51 and the loss : 200463.125\n",
            "Epoch number: 61 and the loss : 200447.578125\n",
            "Epoch number: 71 and the loss : 200430.09375\n",
            "Epoch number: 81 and the loss : 200407.984375\n",
            "Epoch number: 91 and the loss : 200383.890625\n",
            "Epoch number: 101 and the loss : 200358.0625\n",
            "Epoch number: 111 and the loss : 200324.25\n",
            "Epoch number: 121 and the loss : 200291.59375\n",
            "Epoch number: 131 and the loss : 200252.21875\n",
            "Epoch number: 141 and the loss : 200207.671875\n",
            "Epoch number: 151 and the loss : 200161.9375\n",
            "Epoch number: 161 and the loss : 200118.703125\n",
            "Epoch number: 171 and the loss : 200057.453125\n",
            "Epoch number: 181 and the loss : 200008.421875\n",
            "Epoch number: 191 and the loss : 199943.953125\n",
            "Epoch number: 201 and the loss : 199879.015625\n",
            "Epoch number: 211 and the loss : 199821.515625\n",
            "Epoch number: 221 and the loss : 199744.1875\n",
            "Epoch number: 231 and the loss : 199661.890625\n",
            "Epoch number: 241 and the loss : 199588.9375\n",
            "Epoch number: 251 and the loss : 199504.203125\n",
            "Epoch number: 261 and the loss : 199415.71875\n",
            "Epoch number: 271 and the loss : 199330.59375\n",
            "Epoch number: 281 and the loss : 199241.6875\n",
            "Epoch number: 291 and the loss : 199152.53125\n",
            "Epoch number: 301 and the loss : 199062.34375\n",
            "Epoch number: 311 and the loss : 198940.5\n",
            "Epoch number: 321 and the loss : 198822.71875\n",
            "Epoch number: 331 and the loss : 198711.6875\n",
            "Epoch number: 341 and the loss : 198617.953125\n",
            "Epoch number: 351 and the loss : 198499.703125\n",
            "Epoch number: 361 and the loss : 198391.1875\n",
            "Epoch number: 371 and the loss : 198279.546875\n",
            "Epoch number: 381 and the loss : 198110.34375\n",
            "Epoch number: 391 and the loss : 197992.578125\n",
            "Epoch number: 401 and the loss : 197823.0625\n",
            "Epoch number: 411 and the loss : 197748.171875\n",
            "Epoch number: 421 and the loss : 197603.34375\n",
            "Epoch number: 431 and the loss : 197458.59375\n",
            "Epoch number: 441 and the loss : 197357.78125\n",
            "Epoch number: 451 and the loss : 197122.0\n",
            "Epoch number: 461 and the loss : 196978.75\n",
            "Epoch number: 471 and the loss : 196851.203125\n",
            "Epoch number: 481 and the loss : 196692.609375\n",
            "Epoch number: 491 and the loss : 196534.875\n",
            "Epoch number: 501 and the loss : 196369.859375\n",
            "Epoch number: 511 and the loss : 196165.625\n",
            "Epoch number: 521 and the loss : 195965.53125\n",
            "Epoch number: 531 and the loss : 195837.9375\n",
            "Epoch number: 541 and the loss : 195623.5625\n",
            "Epoch number: 551 and the loss : 195496.9375\n",
            "Epoch number: 561 and the loss : 195279.234375\n",
            "Epoch number: 571 and the loss : 195104.359375\n",
            "Epoch number: 581 and the loss : 194846.609375\n",
            "Epoch number: 591 and the loss : 194773.390625\n",
            "Epoch number: 601 and the loss : 194555.234375\n",
            "Epoch number: 611 and the loss : 194387.734375\n",
            "Epoch number: 621 and the loss : 194111.65625\n",
            "Epoch number: 631 and the loss : 193911.421875\n",
            "Epoch number: 641 and the loss : 193664.375\n",
            "Epoch number: 651 and the loss : 193434.125\n",
            "Epoch number: 661 and the loss : 193307.171875\n",
            "Epoch number: 671 and the loss : 193036.984375\n",
            "Epoch number: 681 and the loss : 192904.21875\n",
            "Epoch number: 691 and the loss : 192606.140625\n",
            "Epoch number: 701 and the loss : 192436.3125\n",
            "Epoch number: 711 and the loss : 192161.484375\n",
            "Epoch number: 721 and the loss : 191882.859375\n",
            "Epoch number: 731 and the loss : 191815.0\n",
            "Epoch number: 741 and the loss : 191401.921875\n",
            "Epoch number: 751 and the loss : 191278.703125\n",
            "Epoch number: 761 and the loss : 191026.828125\n",
            "Epoch number: 771 and the loss : 190675.515625\n",
            "Epoch number: 781 and the loss : 190492.0625\n",
            "Epoch number: 791 and the loss : 190253.28125\n",
            "Epoch number: 801 and the loss : 190141.25\n",
            "Epoch number: 811 and the loss : 189889.4375\n",
            "Epoch number: 821 and the loss : 189603.609375\n",
            "Epoch number: 831 and the loss : 189201.546875\n",
            "Epoch number: 841 and the loss : 189100.984375\n",
            "Epoch number: 851 and the loss : 188811.359375\n",
            "Epoch number: 861 and the loss : 188537.140625\n",
            "Epoch number: 871 and the loss : 188185.734375\n",
            "Epoch number: 881 and the loss : 187859.34375\n",
            "Epoch number: 891 and the loss : 187548.78125\n",
            "Epoch number: 901 and the loss : 187486.828125\n",
            "Epoch number: 911 and the loss : 187144.796875\n",
            "Epoch number: 921 and the loss : 186889.359375\n",
            "Epoch number: 931 and the loss : 186507.640625\n",
            "Epoch number: 941 and the loss : 186200.09375\n",
            "Epoch number: 951 and the loss : 185878.4375\n",
            "Epoch number: 961 and the loss : 185642.796875\n",
            "Epoch number: 971 and the loss : 185451.5625\n",
            "Epoch number: 981 and the loss : 184866.15625\n",
            "Epoch number: 991 and the loss : 184723.890625\n",
            "Epoch number: 1001 and the loss : 184387.609375\n",
            "Epoch number: 1011 and the loss : 184209.515625\n",
            "Epoch number: 1021 and the loss : 183740.09375\n",
            "Epoch number: 1031 and the loss : 183716.609375\n",
            "Epoch number: 1041 and the loss : 183266.796875\n",
            "Epoch number: 1051 and the loss : 182873.203125\n",
            "Epoch number: 1061 and the loss : 182641.609375\n",
            "Epoch number: 1071 and the loss : 182317.75\n",
            "Epoch number: 1081 and the loss : 181833.09375\n",
            "Epoch number: 1091 and the loss : 181653.3125\n",
            "Epoch number: 1101 and the loss : 181116.328125\n",
            "Epoch number: 1111 and the loss : 181062.71875\n",
            "Epoch number: 1121 and the loss : 180743.921875\n",
            "Epoch number: 1131 and the loss : 180477.796875\n",
            "Epoch number: 1141 and the loss : 179937.75\n",
            "Epoch number: 1151 and the loss : 179373.28125\n",
            "Epoch number: 1161 and the loss : 179416.421875\n",
            "Epoch number: 1171 and the loss : 178957.375\n",
            "Epoch number: 1181 and the loss : 178698.921875\n",
            "Epoch number: 1191 and the loss : 178043.0\n",
            "Epoch number: 1201 and the loss : 177956.59375\n",
            "Epoch number: 1211 and the loss : 177518.296875\n",
            "Epoch number: 1221 and the loss : 177044.9375\n",
            "Epoch number: 1231 and the loss : 176988.984375\n",
            "Epoch number: 1241 and the loss : 176235.421875\n",
            "Epoch number: 1251 and the loss : 176051.375\n",
            "Epoch number: 1261 and the loss : 175688.171875\n",
            "Epoch number: 1271 and the loss : 175451.6875\n",
            "Epoch number: 1281 and the loss : 175083.6875\n",
            "Epoch number: 1291 and the loss : 174990.8125\n",
            "Epoch number: 1301 and the loss : 174100.21875\n",
            "Epoch number: 1311 and the loss : 173721.375\n",
            "Epoch number: 1321 and the loss : 173633.3125\n",
            "Epoch number: 1331 and the loss : 173315.296875\n",
            "Epoch number: 1341 and the loss : 172433.171875\n",
            "Epoch number: 1351 and the loss : 172225.03125\n",
            "Epoch number: 1361 and the loss : 172128.609375\n",
            "Epoch number: 1371 and the loss : 171520.890625\n",
            "Epoch number: 1381 and the loss : 170934.578125\n",
            "Epoch number: 1391 and the loss : 170942.71875\n",
            "Epoch number: 1401 and the loss : 170339.265625\n",
            "Epoch number: 1411 and the loss : 169717.828125\n",
            "Epoch number: 1421 and the loss : 169888.453125\n",
            "Epoch number: 1431 and the loss : 169664.8125\n",
            "Epoch number: 1441 and the loss : 168945.65625\n",
            "Epoch number: 1451 and the loss : 168638.265625\n",
            "Epoch number: 1461 and the loss : 167846.890625\n",
            "Epoch number: 1471 and the loss : 167823.140625\n",
            "Epoch number: 1481 and the loss : 167424.03125\n",
            "Epoch number: 1491 and the loss : 166547.546875\n",
            "Epoch number: 1501 and the loss : 166302.0625\n",
            "Epoch number: 1511 and the loss : 166306.609375\n",
            "Epoch number: 1521 and the loss : 165751.640625\n",
            "Epoch number: 1531 and the loss : 164794.828125\n",
            "Epoch number: 1541 and the loss : 164770.796875\n",
            "Epoch number: 1551 and the loss : 164034.71875\n",
            "Epoch number: 1561 and the loss : 164031.90625\n",
            "Epoch number: 1571 and the loss : 163543.25\n",
            "Epoch number: 1581 and the loss : 162818.125\n",
            "Epoch number: 1591 and the loss : 162223.96875\n",
            "Epoch number: 1601 and the loss : 161940.34375\n",
            "Epoch number: 1611 and the loss : 161752.84375\n",
            "Epoch number: 1621 and the loss : 160777.453125\n",
            "Epoch number: 1631 and the loss : 161548.515625\n",
            "Epoch number: 1641 and the loss : 160202.65625\n",
            "Epoch number: 1651 and the loss : 160143.859375\n",
            "Epoch number: 1661 and the loss : 159878.140625\n",
            "Epoch number: 1671 and the loss : 159468.171875\n",
            "Epoch number: 1681 and the loss : 158918.921875\n",
            "Epoch number: 1691 and the loss : 158115.765625\n",
            "Epoch number: 1701 and the loss : 157231.25\n",
            "Epoch number: 1711 and the loss : 157171.171875\n",
            "Epoch number: 1721 and the loss : 156632.40625\n",
            "Epoch number: 1731 and the loss : 156769.90625\n",
            "Epoch number: 1741 and the loss : 155916.59375\n",
            "Epoch number: 1751 and the loss : 155629.90625\n",
            "Epoch number: 1761 and the loss : 154945.15625\n",
            "Epoch number: 1771 and the loss : 154669.390625\n",
            "Epoch number: 1781 and the loss : 154183.140625\n",
            "Epoch number: 1791 and the loss : 153729.90625\n",
            "Epoch number: 1801 and the loss : 153992.0625\n",
            "Epoch number: 1811 and the loss : 153329.9375\n",
            "Epoch number: 1821 and the loss : 152037.53125\n",
            "Epoch number: 1831 and the loss : 151585.78125\n",
            "Epoch number: 1841 and the loss : 151306.671875\n",
            "Epoch number: 1851 and the loss : 150244.671875\n",
            "Epoch number: 1861 and the loss : 150325.953125\n",
            "Epoch number: 1871 and the loss : 149849.625\n",
            "Epoch number: 1881 and the loss : 149527.078125\n",
            "Epoch number: 1891 and the loss : 148739.390625\n",
            "Epoch number: 1901 and the loss : 147979.890625\n",
            "Epoch number: 1911 and the loss : 147936.03125\n",
            "Epoch number: 1921 and the loss : 147660.703125\n",
            "Epoch number: 1931 and the loss : 147311.96875\n",
            "Epoch number: 1941 and the loss : 146789.609375\n",
            "Epoch number: 1951 and the loss : 146293.953125\n",
            "Epoch number: 1961 and the loss : 146168.375\n",
            "Epoch number: 1971 and the loss : 145548.78125\n",
            "Epoch number: 1981 and the loss : 145441.515625\n",
            "Epoch number: 1991 and the loss : 144648.71875\n",
            "Epoch number: 2001 and the loss : 144299.703125\n",
            "Epoch number: 2011 and the loss : 143343.28125\n",
            "Epoch number: 2021 and the loss : 142625.59375\n",
            "Epoch number: 2031 and the loss : 143110.234375\n",
            "Epoch number: 2041 and the loss : 143093.203125\n",
            "Epoch number: 2051 and the loss : 141939.46875\n",
            "Epoch number: 2061 and the loss : 141176.796875\n",
            "Epoch number: 2071 and the loss : 140400.96875\n",
            "Epoch number: 2081 and the loss : 139385.1875\n",
            "Epoch number: 2091 and the loss : 139533.5625\n",
            "Epoch number: 2101 and the loss : 139968.25\n",
            "Epoch number: 2111 and the loss : 138505.234375\n",
            "Epoch number: 2121 and the loss : 138352.484375\n",
            "Epoch number: 2131 and the loss : 137530.59375\n",
            "Epoch number: 2141 and the loss : 137466.984375\n",
            "Epoch number: 2151 and the loss : 136506.234375\n",
            "Epoch number: 2161 and the loss : 136836.640625\n",
            "Epoch number: 2171 and the loss : 135118.078125\n",
            "Epoch number: 2181 and the loss : 135767.09375\n",
            "Epoch number: 2191 and the loss : 134253.15625\n",
            "Epoch number: 2201 and the loss : 133860.046875\n",
            "Epoch number: 2211 and the loss : 134002.34375\n",
            "Epoch number: 2221 and the loss : 133329.390625\n",
            "Epoch number: 2231 and the loss : 132574.78125\n",
            "Epoch number: 2241 and the loss : 133304.796875\n",
            "Epoch number: 2251 and the loss : 132329.03125\n",
            "Epoch number: 2261 and the loss : 131776.765625\n",
            "Epoch number: 2271 and the loss : 131205.125\n",
            "Epoch number: 2281 and the loss : 130754.484375\n",
            "Epoch number: 2291 and the loss : 130323.7734375\n",
            "Epoch number: 2301 and the loss : 129882.5078125\n",
            "Epoch number: 2311 and the loss : 129291.3828125\n",
            "Epoch number: 2321 and the loss : 129919.453125\n",
            "Epoch number: 2331 and the loss : 127600.2265625\n",
            "Epoch number: 2341 and the loss : 128522.3984375\n",
            "Epoch number: 2351 and the loss : 126765.890625\n",
            "Epoch number: 2361 and the loss : 126916.9375\n",
            "Epoch number: 2371 and the loss : 125621.3671875\n",
            "Epoch number: 2381 and the loss : 125227.3046875\n",
            "Epoch number: 2391 and the loss : 125116.4296875\n",
            "Epoch number: 2401 and the loss : 124245.6875\n",
            "Epoch number: 2411 and the loss : 123772.3984375\n",
            "Epoch number: 2421 and the loss : 123534.21875\n",
            "Epoch number: 2431 and the loss : 122936.7421875\n",
            "Epoch number: 2441 and the loss : 122203.109375\n",
            "Epoch number: 2451 and the loss : 121879.6171875\n",
            "Epoch number: 2461 and the loss : 121543.7734375\n",
            "Epoch number: 2471 and the loss : 120740.78125\n",
            "Epoch number: 2481 and the loss : 120449.40625\n",
            "Epoch number: 2491 and the loss : 119397.3046875\n",
            "Epoch number: 2501 and the loss : 120559.2109375\n",
            "Epoch number: 2511 and the loss : 119917.328125\n",
            "Epoch number: 2521 and the loss : 119238.2265625\n",
            "Epoch number: 2531 and the loss : 116800.8359375\n",
            "Epoch number: 2541 and the loss : 116886.671875\n",
            "Epoch number: 2551 and the loss : 118234.4453125\n",
            "Epoch number: 2561 and the loss : 116981.28125\n",
            "Epoch number: 2571 and the loss : 115991.953125\n",
            "Epoch number: 2581 and the loss : 115933.3125\n",
            "Epoch number: 2591 and the loss : 114850.875\n",
            "Epoch number: 2601 and the loss : 116344.3515625\n",
            "Epoch number: 2611 and the loss : 113060.34375\n",
            "Epoch number: 2621 and the loss : 112835.0234375\n",
            "Epoch number: 2631 and the loss : 113563.03125\n",
            "Epoch number: 2641 and the loss : 112864.21875\n",
            "Epoch number: 2651 and the loss : 111575.2890625\n",
            "Epoch number: 2661 and the loss : 112154.0625\n",
            "Epoch number: 2671 and the loss : 110752.0390625\n",
            "Epoch number: 2681 and the loss : 111228.6171875\n",
            "Epoch number: 2691 and the loss : 110218.90625\n",
            "Epoch number: 2701 and the loss : 111187.4765625\n",
            "Epoch number: 2711 and the loss : 108178.4140625\n",
            "Epoch number: 2721 and the loss : 108994.625\n",
            "Epoch number: 2731 and the loss : 107772.6796875\n",
            "Epoch number: 2741 and the loss : 106942.828125\n",
            "Epoch number: 2751 and the loss : 107355.015625\n",
            "Epoch number: 2761 and the loss : 105881.1484375\n",
            "Epoch number: 2771 and the loss : 106364.03125\n",
            "Epoch number: 2781 and the loss : 105200.6953125\n",
            "Epoch number: 2791 and the loss : 105280.0703125\n",
            "Epoch number: 2801 and the loss : 103869.125\n",
            "Epoch number: 2811 and the loss : 103216.3984375\n",
            "Epoch number: 2821 and the loss : 104506.2421875\n",
            "Epoch number: 2831 and the loss : 103023.109375\n",
            "Epoch number: 2841 and the loss : 102141.625\n",
            "Epoch number: 2851 and the loss : 101725.3671875\n",
            "Epoch number: 2861 and the loss : 101842.265625\n",
            "Epoch number: 2871 and the loss : 100931.8671875\n",
            "Epoch number: 2881 and the loss : 100845.0\n",
            "Epoch number: 2891 and the loss : 100716.875\n",
            "Epoch number: 2901 and the loss : 99713.5390625\n",
            "Epoch number: 2911 and the loss : 98896.0\n",
            "Epoch number: 2921 and the loss : 98501.6953125\n",
            "Epoch number: 2931 and the loss : 98149.25\n",
            "Epoch number: 2941 and the loss : 98156.140625\n",
            "Epoch number: 2951 and the loss : 96848.1953125\n",
            "Epoch number: 2961 and the loss : 97415.75\n",
            "Epoch number: 2971 and the loss : 96222.4296875\n",
            "Epoch number: 2981 and the loss : 94589.1171875\n",
            "Epoch number: 2991 and the loss : 95828.6953125\n",
            "Epoch number: 3001 and the loss : 94304.8984375\n",
            "Epoch number: 3011 and the loss : 93238.9765625\n",
            "Epoch number: 3021 and the loss : 93391.265625\n",
            "Epoch number: 3031 and the loss : 93549.0859375\n",
            "Epoch number: 3041 and the loss : 92525.8984375\n",
            "Epoch number: 3051 and the loss : 92243.5390625\n",
            "Epoch number: 3061 and the loss : 93733.09375\n",
            "Epoch number: 3071 and the loss : 91719.9609375\n",
            "Epoch number: 3081 and the loss : 91234.4375\n",
            "Epoch number: 3091 and the loss : 90446.0859375\n",
            "Epoch number: 3101 and the loss : 89582.53125\n",
            "Epoch number: 3111 and the loss : 88601.53125\n",
            "Epoch number: 3121 and the loss : 88053.9140625\n",
            "Epoch number: 3131 and the loss : 87950.296875\n",
            "Epoch number: 3141 and the loss : 88317.8515625\n",
            "Epoch number: 3151 and the loss : 87090.046875\n",
            "Epoch number: 3161 and the loss : 87052.75\n",
            "Epoch number: 3171 and the loss : 86062.6875\n",
            "Epoch number: 3181 and the loss : 86195.8984375\n",
            "Epoch number: 3191 and the loss : 84459.828125\n",
            "Epoch number: 3201 and the loss : 85526.84375\n",
            "Epoch number: 3211 and the loss : 83597.4140625\n",
            "Epoch number: 3221 and the loss : 84228.984375\n",
            "Epoch number: 3231 and the loss : 83427.234375\n",
            "Epoch number: 3241 and the loss : 84702.203125\n",
            "Epoch number: 3251 and the loss : 81476.65625\n",
            "Epoch number: 3261 and the loss : 82529.1953125\n",
            "Epoch number: 3271 and the loss : 82073.7578125\n",
            "Epoch number: 3281 and the loss : 80814.3125\n",
            "Epoch number: 3291 and the loss : 80100.796875\n",
            "Epoch number: 3301 and the loss : 79702.40625\n",
            "Epoch number: 3311 and the loss : 79553.75\n",
            "Epoch number: 3321 and the loss : 79252.671875\n",
            "Epoch number: 3331 and the loss : 78483.4453125\n",
            "Epoch number: 3341 and the loss : 78212.5234375\n",
            "Epoch number: 3351 and the loss : 77193.6171875\n",
            "Epoch number: 3361 and the loss : 76943.03125\n",
            "Epoch number: 3371 and the loss : 76026.5625\n",
            "Epoch number: 3381 and the loss : 76268.625\n",
            "Epoch number: 3391 and the loss : 75643.34375\n",
            "Epoch number: 3401 and the loss : 74524.640625\n",
            "Epoch number: 3411 and the loss : 73951.765625\n",
            "Epoch number: 3421 and the loss : 73528.734375\n",
            "Epoch number: 3431 and the loss : 74031.84375\n",
            "Epoch number: 3441 and the loss : 72599.9375\n",
            "Epoch number: 3451 and the loss : 72083.1484375\n",
            "Epoch number: 3461 and the loss : 72078.2421875\n",
            "Epoch number: 3471 and the loss : 71157.6875\n",
            "Epoch number: 3481 and the loss : 70813.7421875\n",
            "Epoch number: 3491 and the loss : 69402.21875\n",
            "Epoch number: 3501 and the loss : 69008.5546875\n",
            "Epoch number: 3511 and the loss : 70029.015625\n",
            "Epoch number: 3521 and the loss : 69532.8359375\n",
            "Epoch number: 3531 and the loss : 68305.4296875\n",
            "Epoch number: 3541 and the loss : 68147.3046875\n",
            "Epoch number: 3551 and the loss : 68163.1328125\n",
            "Epoch number: 3561 and the loss : 65907.625\n",
            "Epoch number: 3571 and the loss : 66757.1484375\n",
            "Epoch number: 3581 and the loss : 65935.3671875\n",
            "Epoch number: 3591 and the loss : 66264.1875\n",
            "Epoch number: 3601 and the loss : 65763.765625\n",
            "Epoch number: 3611 and the loss : 65398.27734375\n",
            "Epoch number: 3621 and the loss : 63766.515625\n",
            "Epoch number: 3631 and the loss : 63394.25\n",
            "Epoch number: 3641 and the loss : 62805.0390625\n",
            "Epoch number: 3651 and the loss : 62424.5703125\n",
            "Epoch number: 3661 and the loss : 62494.0859375\n",
            "Epoch number: 3671 and the loss : 65300.83984375\n",
            "Epoch number: 3681 and the loss : 61633.1953125\n",
            "Epoch number: 3691 and the loss : 61423.296875\n",
            "Epoch number: 3701 and the loss : 60652.21875\n",
            "Epoch number: 3711 and the loss : 60906.28515625\n",
            "Epoch number: 3721 and the loss : 60792.30078125\n",
            "Epoch number: 3731 and the loss : 60365.21484375\n",
            "Epoch number: 3741 and the loss : 57995.67578125\n",
            "Epoch number: 3751 and the loss : 58802.02734375\n",
            "Epoch number: 3761 and the loss : 58038.54296875\n",
            "Epoch number: 3771 and the loss : 57358.66015625\n",
            "Epoch number: 3781 and the loss : 57310.12109375\n",
            "Epoch number: 3791 and the loss : 56454.7421875\n",
            "Epoch number: 3801 and the loss : 57154.75\n",
            "Epoch number: 3811 and the loss : 56588.25\n",
            "Epoch number: 3821 and the loss : 55643.64453125\n",
            "Epoch number: 3831 and the loss : 53451.38671875\n",
            "Epoch number: 3841 and the loss : 53713.35546875\n",
            "Epoch number: 3851 and the loss : 52993.640625\n",
            "Epoch number: 3861 and the loss : 54562.0703125\n",
            "Epoch number: 3871 and the loss : 54110.74609375\n",
            "Epoch number: 3881 and the loss : 55814.05859375\n",
            "Epoch number: 3891 and the loss : 52049.03515625\n",
            "Epoch number: 3901 and the loss : 51388.67578125\n",
            "Epoch number: 3911 and the loss : 51559.9609375\n",
            "Epoch number: 3921 and the loss : 49998.63671875\n",
            "Epoch number: 3931 and the loss : 51229.91796875\n",
            "Epoch number: 3941 and the loss : 50728.82421875\n",
            "Epoch number: 3951 and the loss : 49690.2421875\n",
            "Epoch number: 3961 and the loss : 49820.1484375\n",
            "Epoch number: 3971 and the loss : 48867.40625\n",
            "Epoch number: 3981 and the loss : 49240.97265625\n",
            "Epoch number: 3991 and the loss : 49544.3984375\n",
            "Epoch number: 4001 and the loss : 48418.7734375\n",
            "Epoch number: 4011 and the loss : 48526.12890625\n",
            "Epoch number: 4021 and the loss : 48875.875\n",
            "Epoch number: 4031 and the loss : 47270.03125\n",
            "Epoch number: 4041 and the loss : 47697.796875\n",
            "Epoch number: 4051 and the loss : 46371.5234375\n",
            "Epoch number: 4061 and the loss : 45073.58984375\n",
            "Epoch number: 4071 and the loss : 45254.515625\n",
            "Epoch number: 4081 and the loss : 45841.6875\n",
            "Epoch number: 4091 and the loss : 45942.46484375\n",
            "Epoch number: 4101 and the loss : 45941.37109375\n",
            "Epoch number: 4111 and the loss : 45471.1015625\n",
            "Epoch number: 4121 and the loss : 45007.16796875\n",
            "Epoch number: 4131 and the loss : 45418.69921875\n",
            "Epoch number: 4141 and the loss : 43526.95703125\n",
            "Epoch number: 4151 and the loss : 44990.265625\n",
            "Epoch number: 4161 and the loss : 42843.8828125\n",
            "Epoch number: 4171 and the loss : 43770.91796875\n",
            "Epoch number: 4181 and the loss : 46504.43359375\n",
            "Epoch number: 4191 and the loss : 41836.55078125\n",
            "Epoch number: 4201 and the loss : 42501.4609375\n",
            "Epoch number: 4211 and the loss : 43471.31640625\n",
            "Epoch number: 4221 and the loss : 42180.65625\n",
            "Epoch number: 4231 and the loss : 41725.89453125\n",
            "Epoch number: 4241 and the loss : 41883.08984375\n",
            "Epoch number: 4251 and the loss : 41338.30078125\n",
            "Epoch number: 4261 and the loss : 42754.37890625\n",
            "Epoch number: 4271 and the loss : 40472.0625\n",
            "Epoch number: 4281 and the loss : 39029.4609375\n",
            "Epoch number: 4291 and the loss : 42106.0859375\n",
            "Epoch number: 4301 and the loss : 39642.3046875\n",
            "Epoch number: 4311 and the loss : 41152.7578125\n",
            "Epoch number: 4321 and the loss : 39761.640625\n",
            "Epoch number: 4331 and the loss : 40184.3671875\n",
            "Epoch number: 4341 and the loss : 39517.796875\n",
            "Epoch number: 4351 and the loss : 41733.58984375\n",
            "Epoch number: 4361 and the loss : 39895.69921875\n",
            "Epoch number: 4371 and the loss : 39117.76953125\n",
            "Epoch number: 4381 and the loss : 39106.67578125\n",
            "Epoch number: 4391 and the loss : 40449.1328125\n",
            "Epoch number: 4401 and the loss : 37949.0625\n",
            "Epoch number: 4411 and the loss : 38158.46875\n",
            "Epoch number: 4421 and the loss : 39361.359375\n",
            "Epoch number: 4431 and the loss : 37471.33984375\n",
            "Epoch number: 4441 and the loss : 38351.8828125\n",
            "Epoch number: 4451 and the loss : 37953.08203125\n",
            "Epoch number: 4461 and the loss : 37979.76171875\n",
            "Epoch number: 4471 and the loss : 39679.37890625\n",
            "Epoch number: 4481 and the loss : 39197.96875\n",
            "Epoch number: 4491 and the loss : 38061.60546875\n",
            "Epoch number: 4501 and the loss : 36594.69921875\n",
            "Epoch number: 4511 and the loss : 38811.75390625\n",
            "Epoch number: 4521 and the loss : 37350.14453125\n",
            "Epoch number: 4531 and the loss : 36677.9375\n",
            "Epoch number: 4541 and the loss : 38107.87109375\n",
            "Epoch number: 4551 and the loss : 38389.484375\n",
            "Epoch number: 4561 and the loss : 38955.3359375\n",
            "Epoch number: 4571 and the loss : 35895.59375\n",
            "Epoch number: 4581 and the loss : 37336.69921875\n",
            "Epoch number: 4591 and the loss : 37466.15625\n",
            "Epoch number: 4601 and the loss : 39875.74609375\n",
            "Epoch number: 4611 and the loss : 37391.96484375\n",
            "Epoch number: 4621 and the loss : 35312.91796875\n",
            "Epoch number: 4631 and the loss : 36635.28515625\n",
            "Epoch number: 4641 and the loss : 35939.96484375\n",
            "Epoch number: 4651 and the loss : 36548.328125\n",
            "Epoch number: 4661 and the loss : 36727.875\n",
            "Epoch number: 4671 and the loss : 37130.97265625\n",
            "Epoch number: 4681 and the loss : 35156.83984375\n",
            "Epoch number: 4691 and the loss : 36764.6015625\n",
            "Epoch number: 4701 and the loss : 38684.02734375\n",
            "Epoch number: 4711 and the loss : 35239.15625\n",
            "Epoch number: 4721 and the loss : 34772.1015625\n",
            "Epoch number: 4731 and the loss : 36370.80078125\n",
            "Epoch number: 4741 and the loss : 36112.33984375\n",
            "Epoch number: 4751 and the loss : 34914.1484375\n",
            "Epoch number: 4761 and the loss : 35121.85546875\n",
            "Epoch number: 4771 and the loss : 34950.18359375\n",
            "Epoch number: 4781 and the loss : 39267.99609375\n",
            "Epoch number: 4791 and the loss : 37227.52734375\n",
            "Epoch number: 4801 and the loss : 35871.9921875\n",
            "Epoch number: 4811 and the loss : 34639.03515625\n",
            "Epoch number: 4821 and the loss : 36266.18359375\n",
            "Epoch number: 4831 and the loss : 39634.23046875\n",
            "Epoch number: 4841 and the loss : 36654.51953125\n",
            "Epoch number: 4851 and the loss : 37205.80078125\n",
            "Epoch number: 4861 and the loss : 36081.6875\n",
            "Epoch number: 4871 and the loss : 36819.1171875\n",
            "Epoch number: 4881 and the loss : 37500.46484375\n",
            "Epoch number: 4891 and the loss : 35488.80859375\n",
            "Epoch number: 4901 and the loss : 35982.58203125\n",
            "Epoch number: 4911 and the loss : 36439.84765625\n",
            "Epoch number: 4921 and the loss : 37330.65234375\n",
            "Epoch number: 4931 and the loss : 35747.0078125\n",
            "Epoch number: 4941 and the loss : 34200.45703125\n",
            "Epoch number: 4951 and the loss : 36329.37890625\n",
            "Epoch number: 4961 and the loss : 35786.1328125\n",
            "Epoch number: 4971 and the loss : 35539.59765625\n",
            "Epoch number: 4981 and the loss : 38352.625\n",
            "Epoch number: 4991 and the loss : 34531.671875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Blbyk9If2Fr5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "aaa75fe9-9fb4-46e8-80b2-7d3ef5eae610"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.plot(range(epochs), final_losses)\n",
        "plt.ylabel('RMSE Loss')\n",
        "plt.xlabel('epoch');"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOiLB4E72dd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Thank you!!!"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}